{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41b7905f-a070-4ffe-abfc-67fbcd2adaa9",
      "metadata": {
        "id": "41b7905f-a070-4ffe-abfc-67fbcd2adaa9"
      },
      "source": [
        "## TC 5033\n",
        "## Deep Learning\n",
        "## Transformers\n",
        "\n",
        "#### Activity 4: Implementing a Translator\n",
        "\n",
        "- Objective\n",
        "\n",
        "To understand the Transformer Architecture by Implementing a translator.\n",
        "\n",
        "- Instructions\n",
        "\n",
        "    This activity requires submission in teams. While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
        "\n",
        "    Follow the provided code. The code already implements a transformer from scratch as explained in one of [week's 9 videos](https://youtu.be/XefFj4rLHgU)\n",
        "\n",
        "    Since the provided code already implements a simple translator, your job for this assignment is to understand it fully, and document it using pictures, figures, and markdown cells.  You should test your translator with at least 10 sentences. The dataset used for this task was obtained from [Tatoeba, a large dataset of sentences and translations](https://tatoeba.org/en/downloads).\n",
        "  \n",
        "- Evaluation Criteria\n",
        "\n",
        "    - Code Readability and Comments\n",
        "    - Traning a translator\n",
        "    - Translating at least 10 sentences.\n",
        "\n",
        "- Submission\n",
        "\n",
        "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f240f0d8-d9e0-4632-962f-1a5a7881cb5f",
      "metadata": {
        "id": "f240f0d8-d9e0-4632-962f-1a5a7881cb5f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ebcf8f0-fcf0-4cf7-a549-0c68aa8eab1a",
      "metadata": {
        "id": "5ebcf8f0-fcf0-4cf7-a549-0c68aa8eab1a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "17f54c65",
      "metadata": {
        "heading_collapsed": true,
        "id": "17f54c65"
      },
      "source": [
        "#### Script to convert csv to text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f02c0c2",
      "metadata": {
        "hidden": true,
        "id": "8f02c0c2"
      },
      "outputs": [],
      "source": [
        "#This script requires to convert the TSV file to CSV\n",
        "# easiest way is to open it in Calc or excel and save as csv\n",
        "PATH = '/content/Sentencee.csv'\n",
        "import pandas as pd\n",
        "df = pd.read_csv(PATH, sep='\\t', on_bad_lines='skip', engine='python')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787d9408",
      "metadata": {
        "hidden": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "787d9408",
        "outputId": "6019396f-1c5a-46a9-c541-4065bba554e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-47efff290c33>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  eng_spa_cols['length'] = eng_spa_cols.iloc[:, 0].str.len()\n"
          ]
        }
      ],
      "source": [
        "eng_spa_cols = df.iloc[:, [1, 3]]\n",
        "eng_spa_cols['length'] = eng_spa_cols.iloc[:, 0].str.len()\n",
        "eng_spa_cols = eng_spa_cols.sort_values(by='length')\n",
        "eng_spa_cols = eng_spa_cols.drop(columns=['length'])\n",
        "\n",
        "output_file_path = '/content/sample_data/eng-spa4.txt'\n",
        "eng_spa_cols.to_csv(output_file_path, sep='\\t', index=False, header=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d468e9a",
      "metadata": {
        "id": "7d468e9a"
      },
      "source": [
        "## Transformer - Attention is all you need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5dcf681",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5dcf681",
        "outputId": "bea0198c-172a-4ab6-9d9e-a39c2b6e357a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79b8675554b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "torch.manual_seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2cbd17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c2cbd17",
        "outputId": "6c4f0fec-8a54-4929-86fc-a385b8dbfa59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c6623a1",
      "metadata": {
        "id": "9c6623a1"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LEN = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer-Based Model\n",
        "\n",
        "This code implements the core components of a Transformer model, specifically the **Encoder** and **Decoder** layers, as well as supporting elements like positional embeddings, multi-head attention, and feed-forward layers. Below, we provide detailed documentation along with inline comments to explain each part of the code.\n",
        "\n",
        "## Table of Contents\n",
        "###1. PositionalEmbedding\n",
        "Role: Adds information about the position of each token in the input sequence. Since the model doesn't inherently understand order, these embeddings help it learn the sequence structure.\n",
        "How: Uses sine and cosine functions to generate unique positional vectors for each position in the input sequence.\n",
        "\n",
        "###2. MultiHeadAttention\n",
        "Role: Allows the model to focus on different parts of the input sequence simultaneously (multi-heads), improving its ability to understand complex relationships.\n",
        "How: Uses three different vectors—Query (Q), Key (K), and Value (V)—to compute attention scores and generate weighted values. This process is repeated with multiple attention heads for better learning.\n",
        "\n",
        "###3. PositionFeedForward\n",
        "Role: Processes the output of the attention layer to capture more complex patterns in the data.\n",
        "How: A simple feed-forward network that applies two linear transformations with a ReLU activation in between. It helps the model learn non-linear transformations.\n",
        "\n",
        "###4. EncoderSubLayer\n",
        "Role: A single processing unit inside the encoder that includes a self-attention mechanism and a feed-forward network.\n",
        "How: The input goes through self-attention (to learn relationships between tokens) and then through a feed-forward network (to further process the information). Each step has residual connections and layer normalization for stability.\n",
        "\n",
        "###5. Encoder\n",
        "Role: Stacks multiple EncoderSubLayers to process the entire input sequence and generate an encoded representation of it.\n",
        "How: The input is passed through each EncoderSubLayer sequentially. The output is the final, processed sequence that captures rich information about the input.\n",
        "###6. DecoderSubLayer\n",
        "\n",
        "Role: A single processing unit inside the decoder that handles both self-attention (within the target sequence) and cross-attention (with the encoder’s output).\n",
        "How: First, it applies self-attention on the target sequence, then applies cross-attention to combine information from the encoder's output, followed by a feed-forward network. It has residual connections and normalization for each step.\n",
        "\n",
        "###7. Decoder\n",
        "Role: Stacks multiple DecoderSubLayers to generate the final output sequence (like translation or prediction) based on the encoded input.\n",
        "\n",
        "How: The decoder receives the encoder's output and the current target sequence, applying multiple DecoderSubLayers to generate a refined sequence, often for tasks like translation or text generation."
      ],
      "metadata": {
        "id": "KTfnXVShnYLe"
      },
      "id": "KTfnXVShnYLe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3103d45f",
      "metadata": {
        "code_folding": [
          30,
          94
        ],
        "id": "3103d45f"
      },
      "outputs": [],
      "source": [
        "### 1. PositionalEmbedding\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=MAX_SEQ_LEN):\n",
        "        \"\"\"\n",
        "        Initializes a matrix of sinusoidal and cosinusoidal positional embeddings.\n",
        "        These embeddings encode positional information, enabling the model to differentiate\n",
        "        between token positions in a sequence.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimension of the model's embeddings.\n",
        "            max_seq_len (int): The maximum sequence length for which positional encodings\n",
        "                               are computed.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a matrix to store the positional embeddings for each token position,\n",
        "        # of shape (max_seq_len, d_model)\n",
        "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
        "\n",
        "        # Generate positions as a tensor, with each position represented as an integer from 0\n",
        "        # to max_seq_len - 1. Reshape to (max_seq_len, 1) to prepare for broadcasting.\n",
        "        token_pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Calculate a scaling factor for each dimension in d_model, using the formula\n",
        "        # exp(-log(10000) * (2i / d_model)), which scales down higher dimensions.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sinusoidal and cosinusoidal transformations for even and odd indices:\n",
        "        # sin for dimensions 0, 2, 4, ... (even indices)\n",
        "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
        "        # cos for dimensions 1, 3, 5, ... (odd indices)\n",
        "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
        "\n",
        "        # Add an extra dimension to allow broadcasting over a batch, and transpose to shape\n",
        "        # (max_seq_len, 1, d_model) for easier addition to input embeddings in forward pass.\n",
        "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Adds positional embeddings to the input embeddings.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input embeddings of shape [seq_len, batch_size, d_model].\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The input embeddings with positional encodings added, of the same shape\n",
        "                    [seq_len, batch_size, d_model].\n",
        "        \"\"\"\n",
        "        # Add the positional embedding matrix to the input embeddings.\n",
        "        # Only the first 'seq_len' positions are used to match the input sequence length.\n",
        "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
        "\n",
        "\n",
        "### 2. MultiHeadAttention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model=512, num_heads=8):\n",
        "        \"\"\"\n",
        "        Initializes multi-head attention by setting up the query, key, value,\n",
        "        and output projections across multiple attention heads.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimensionality of the input embeddings.\n",
        "            num_heads (int): The number of attention heads.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Ensure d_model is divisible by num_heads\n",
        "        assert d_model % num_heads == 0, 'Embedding size must be divisible by the number of heads'\n",
        "\n",
        "        # Define the dimension per head for query/key/value projections\n",
        "        self.d_v = d_model // num_heads  # Dimensionality for values\n",
        "        self.d_k = self.d_v              # Dimensionality for keys (usually equal to d_v)\n",
        "        self.num_heads = num_heads       # Total number of attention heads\n",
        "\n",
        "        # Linear transformations for query, key, and value matrices\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linear transformation for the concatenated output from all heads\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Applies multi-head attention to the input query, key, and value matrices.\n",
        "\n",
        "        Args:\n",
        "            Q (Tensor): Query matrix of shape [batch_size, seq_len, d_model].\n",
        "            K (Tensor): Key matrix of shape [batch_size, seq_len, d_model].\n",
        "            V (Tensor): Value matrix of shape [batch_size, seq_len, d_model].\n",
        "            mask (Tensor, optional): Optional mask to control attention focus. Default: None\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output from multi-head attention, of shape [batch_size, seq_len, d_model].\n",
        "            Tensor: Attention weights across heads, of shape [batch_size, num_heads, seq_len, seq_len].\n",
        "        \"\"\"\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        # Project Q, K, V to shape [batch_size, num_heads, seq_len, d_k]\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Apply scaled dot-product attention to each head\n",
        "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
        "\n",
        "        # Concatenate all heads back into a single matrix and project through W_o\n",
        "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        return self.W_o(weighted_values), attention\n",
        "\n",
        "    def scale_dot_product(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Computes the scaled dot-product attention scores and applies a softmax to get attention weights.\n",
        "\n",
        "        Args:\n",
        "            Q (Tensor): Query tensor of shape [batch_size, num_heads, seq_len, d_k].\n",
        "            K (Tensor): Key tensor of shape [batch_size, num_heads, seq_len, d_k].\n",
        "            V (Tensor): Value tensor of shape [batch_size, num_heads, seq_len, d_k].\n",
        "            mask (Tensor, optional): Optional mask for attention, of shape [batch_size, num_heads, seq_len, seq_len].\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output of shape [batch_size, num_heads, seq_len, d_k] after attention is applied.\n",
        "            Tensor: Attention weights of shape [batch_size, num_heads, seq_len, seq_len].\n",
        "        \"\"\"\n",
        "        # Calculate dot products between Q and K, then scale by sqrt(d_k)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # If mask is provided, apply it to scores (set masked positions to -inf)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Compute attention weights by applying softmax to scores\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Weighted sum of the values based on attention weights\n",
        "        weighted_values = torch.matmul(attention, V)\n",
        "\n",
        "        return weighted_values, attention\n",
        "\n",
        "\n",
        "### 3. PositionFeedForward\n",
        "\n",
        "class PositionFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        \"\"\"\n",
        "        Initializes a feed-forward network with two linear layers.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): Input and output embedding dimension.\n",
        "            d_ff (int): Dimension of the hidden layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply feed-forward transformation with ReLU activation\n",
        "        return self.linear2(F.relu(self.linear1(x)))\n",
        "\n",
        "### 4. EncoderSubLayer\n",
        "class EncoderSubLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initializes a single encoder sub-layer that includes a multi-head self-attention mechanism\n",
        "        and a feed-forward neural network.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): Dimensionality of the model.\n",
        "            num_heads (int): Number of attention heads in the self-attention mechanism.\n",
        "            d_ff (int): Dimension of the feed-forward network.\n",
        "            dropout (float): Dropout probability for regularization.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head self-attention layer\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Position-wise feed-forward network\n",
        "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
        "\n",
        "        # Layer normalization applied after each sub-layer\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout layers to prevent overfitting\n",
        "        self.droupout1 = nn.Dropout(dropout)\n",
        "        self.droupout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the encoder sub-layer.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, seq_len, d_model].\n",
        "            mask (Tensor, optional): Mask for attention weights. Default: None\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape [batch_size, seq_len, d_model].\n",
        "        \"\"\"\n",
        "        # Apply multi-head self-attention mechanism\n",
        "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
        "\n",
        "        # Add and normalize: add attention output to input (residual connection)\n",
        "        x = x + self.droupout1(attention_score)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        ffn_output = self.ffn(x)\n",
        "\n",
        "        # Add and normalize: add feed-forward output to previous output\n",
        "        x = x + self.droupout2(ffn_output)\n",
        "        return self.norm2(x)\n",
        "\n",
        "\n",
        "### 5. Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the encoder module, which consists of multiple encoder sub-layers.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): Dimensionality of the model.\n",
        "            num_heads (int): Number of attention heads in each sub-layer.\n",
        "            d_ff (int): Dimension of the feed-forward network in each sub-layer.\n",
        "            num_layers (int): Number of encoder sub-layers in the encoder.\n",
        "            dropout (float): Dropout probability for regularization.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Stack of encoder sub-layers\n",
        "        self.layers = nn.ModuleList(\n",
        "            [EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        # Final layer normalization after the last encoder layer\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the encoder stack.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, seq_len, d_model].\n",
        "            mask (Tensor, optional): Mask to control attention. Default: None\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Encoded output of shape [batch_size, seq_len, d_model].\n",
        "        \"\"\"\n",
        "        # Pass input through each encoder sub-layer\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # Apply normalization after the final encoder sub-layer\n",
        "        return self.norm(x)\n",
        "\n",
        "### 6. DecoderSubLayer\n",
        "class DecoderSubLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initializes a Decoder sub-layer with self-attention, cross-attention, and\n",
        "        feed-forward components. Each of these is followed by layer normalization and dropout.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): The dimension of input embeddings and the hidden layer size.\n",
        "            num_heads (int): Number of attention heads in multi-head attention layers.\n",
        "            d_ff (int): Dimensionality of the intermediate feed-forward layer.\n",
        "            dropout (float): Dropout rate for regularization.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Self-attention layer for attending to the target sequence itself\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Cross-attention layer for attending to the encoder's output (source sequence)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Feed-forward network for additional non-linear transformations\n",
        "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
        "\n",
        "        # Layer normalizations for stabilizing training and normalizing intermediate representations\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout layers for regularization after each sub-layer\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for the Decoder sub-layer.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor representing the current state of the target sequence,\n",
        "                        of shape [batch_size, target_seq_len, d_model].\n",
        "            encoder_output (Tensor): Output tensor from the encoder, representing the\n",
        "                                     encoded source sequence, of shape [batch_size, src_seq_len, d_model].\n",
        "            target_mask (Tensor): Optional mask tensor for self-attention, used to mask out\n",
        "                                  future positions for autoregressive training.\n",
        "            encoder_mask (Tensor): Optional mask tensor for cross-attention, used to mask\n",
        "                                   out padding in the source sequence.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor after applying self-attention, cross-attention,\n",
        "                    and feed-forward operations, with layer normalization and dropout.\n",
        "        \"\"\"\n",
        "\n",
        "        # --- Self-Attention Block ---\n",
        "        # Apply self-attention over the target sequence, allowing each position to attend\n",
        "        # to previous positions up to itself. The optional target_mask ensures that attention\n",
        "        # is only paid to past and present tokens.\n",
        "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
        "\n",
        "        # Add residual connection and apply dropout and normalization\n",
        "        x = x + self.dropout1(attention_score)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # --- Cross-Attention Block ---\n",
        "        # Apply cross-attention to allow the decoder to attend to the encoder's output.\n",
        "        # Here, the decoder uses the encoder's representations as keys and values, allowing\n",
        "        # each position in the target sequence to attend to the source sequence.\n",
        "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n",
        "\n",
        "        # Add residual connection and apply dropout and normalization\n",
        "        x = x + self.dropout2(encoder_attn)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # --- Feed-Forward Network Block ---\n",
        "        # Pass the output through a feed-forward network, adding non-linearity and additional\n",
        "        # capacity to the model for complex transformations.\n",
        "        ff_output = self.feed_forward(x)\n",
        "\n",
        "        # Add residual connection, dropout, and final normalization\n",
        "        x = x + self.dropout3(ff_output)\n",
        "        return self.norm3(x)\n",
        "\n",
        "\n",
        "### 7. Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initializes a Decoder, which consists of a series of DecoderSubLayer layers and a\n",
        "        final layer normalization.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): Dimension of the input embeddings and the hidden layer size.\n",
        "            num_heads (int): Number of attention heads in each multi-head attention layer.\n",
        "            d_ff (int): Dimensionality of the intermediate feed-forward network.\n",
        "            num_layers (int): Number of DecoderSubLayer layers in the Decoder.\n",
        "            dropout (float): Dropout rate for each sublayer in the Decoder.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Stack of DecoderSubLayer instances, each with its own multi-head attention, cross-attention,\n",
        "        # and feed-forward networks, dropout, and normalization.\n",
        "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout)\n",
        "                                     for _ in range(num_layers)])\n",
        "\n",
        "        # Final layer normalization after all decoder sublayers\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
        "        \"\"\"\n",
        "        Forward pass through the Decoder.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, target_seq_len, d_model] representing the\n",
        "                        target sequence embeddings.\n",
        "            encoder_output (Tensor): Output tensor from the encoder of shape\n",
        "                                     [batch_size, src_seq_len, d_model].\n",
        "            target_mask (Tensor): Mask for self-attention, which prevents attending to future positions.\n",
        "            encoder_mask (Tensor): Mask for cross-attention, which prevents attending to padding\n",
        "                                   tokens in the encoder output.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape [batch_size, target_seq_len, d_model] after passing through\n",
        "                    the entire stack of DecoderSubLayer layers and the final normalization.\n",
        "        \"\"\"\n",
        "\n",
        "        # Pass through each layer in the Decoder stack\n",
        "        for layer in self.layers:\n",
        "            # Each DecoderSubLayer layer processes the input 'x' and the encoder output,\n",
        "            # applying self-attention, cross-attention, and a feed-forward layer\n",
        "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
        "\n",
        "        # Apply final layer normalization to stabilize the output\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61070162",
      "metadata": {
        "code_folding": [],
        "id": "61070162"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
        "                 input_vocab_size, target_vocab_size,\n",
        "                 max_len=MAX_SEQ_LEN, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
        "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
        "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
        "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        # Encoder mask\n",
        "        source_mask, target_mask = self.mask(source, target)\n",
        "        # Embedding and positional Encoding\n",
        "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
        "        source = self.pos_embedding(source)\n",
        "        # Encoder\n",
        "        encoder_output = self.encoder(source, source_mask)\n",
        "\n",
        "        # Decoder embedding and postional encoding\n",
        "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
        "        target = self.pos_embedding(target)\n",
        "        # Decoder\n",
        "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
        "\n",
        "        return self.output_layer(output)\n",
        "\n",
        "\n",
        "\n",
        "    def mask(self, source, target):\n",
        "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
        "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
        "        size = target.size(1)\n",
        "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()\n",
        "        target_mask = target_mask & no_mask\n",
        "        return source_mask, target_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da6b2d4",
      "metadata": {
        "heading_collapsed": true,
        "id": "6da6b2d4"
      },
      "source": [
        "#### Simple test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d40581d6",
      "metadata": {
        "hidden": true,
        "id": "d40581d6"
      },
      "outputs": [],
      "source": [
        "seq_len_source = 10\n",
        "seq_len_target = 10\n",
        "batch_size = 2\n",
        "input_vocab_size = 50\n",
        "target_vocab_size = 50\n",
        "\n",
        "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n",
        "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc7cf689",
      "metadata": {
        "hidden": true,
        "id": "fc7cf689"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "num_layers = 6\n",
        "\n",
        "model = Transformer(d_model, num_heads, d_ff, num_layers,\n",
        "                  input_vocab_size, target_vocab_size,\n",
        "                  max_len=MAX_SEQ_LEN, dropout=0.1)\n",
        "\n",
        "model = model.to(device)\n",
        "source = source.to(device)\n",
        "target = target.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4618560e",
      "metadata": {
        "hidden": true,
        "id": "4618560e"
      },
      "outputs": [],
      "source": [
        "output = model(source, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0bc69d",
      "metadata": {
        "hidden": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab0bc69d",
        "outputId": "88a7f8cb-9e23-4db0-c2fe-73fc0349f63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ouput.shape torch.Size([2, 10, 50])\n"
          ]
        }
      ],
      "source": [
        "# Expected output shape -> [batch, seq_len_target, target_vocab_size] i.e. [2, 10, 50]\n",
        "print(f'ouput.shape {output.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f4b2910",
      "metadata": {
        "id": "0f4b2910"
      },
      "source": [
        "### Translator Eng-Spa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "869a7244",
      "metadata": {
        "id": "869a7244"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/sample_data/eng-spa4.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0af1eba",
      "metadata": {
        "id": "d0af1eba"
      },
      "outputs": [],
      "source": [
        "with open(PATH, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "eng_spa_pairs = [line.strip().split('\\t') for line in lines if '\\t' in line]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c930226f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c930226f",
        "outputId": "486f591c-fc9f-4541-f7df-610d8c03d365"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Go.', 'Vaya.'],\n",
              " ['Go.', 'Ve.'],\n",
              " ['Hi.', '¡Hola!'],\n",
              " ['So?', '¿Y?'],\n",
              " ['Ok!', '¡OK!'],\n",
              " ['OK.', '¡Órale!'],\n",
              " ['Ah!', '¡Anda!'],\n",
              " ['Hi.', 'Hola.'],\n",
              " ['Go!', '¡Fuera!'],\n",
              " ['Go!', '¡Ya!']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "eng_spa_pairs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095f4037",
      "metadata": {
        "id": "095f4037"
      },
      "outputs": [],
      "source": [
        "eng_sentences = [pair[0] for pair in eng_spa_pairs]\n",
        "spa_sentences = [pair[1] for pair in eng_spa_pairs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d9e1c95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d9e1c95",
        "outputId": "27381a16-18b7-49cc-f34d-fc7d19c11de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Go.', 'Go.', 'Hi.', 'So?', 'Ok!', 'OK.', 'Ah!', 'Hi.', 'Go!', 'Go!']\n",
            "['Vaya.', 'Ve.', '¡Hola!', '¿Y?', '¡OK!', '¡Órale!', '¡Anda!', 'Hola.', '¡Fuera!', '¡Ya!']\n"
          ]
        }
      ],
      "source": [
        "print(eng_sentences[:10])\n",
        "print(spa_sentences[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d11478",
      "metadata": {
        "id": "60d11478"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n",
        "    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n",
        "    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n",
        "    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n",
        "    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n",
        "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    sentence = '<sos> ' + sentence + ' <eos>'\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "478f673b",
      "metadata": {
        "id": "478f673b"
      },
      "outputs": [],
      "source": [
        "s1 = '¿Hola @ cómo estás? 123'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ac79c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ac79c5",
        "outputId": "628698fd-a1a6-45bc-f398-9c5c993e37ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Hola @ cómo estás? 123\n",
            "<sos> hola como estas <eos>\n"
          ]
        }
      ],
      "source": [
        "print(s1)\n",
        "print(preprocess_sentence(s1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9fc9c4d",
      "metadata": {
        "id": "d9fc9c4d"
      },
      "outputs": [],
      "source": [
        "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
        "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a3b18d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7a3b18d",
        "outputId": "e004b2e7-1d8e-492b-c947-9a1cd6c791f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos> vaya <eos>',\n",
              " '<sos> ve <eos>',\n",
              " '<sos> hola <eos>',\n",
              " '<sos> y <eos>',\n",
              " '<sos> ok <eos>',\n",
              " '<sos> orale <eos>',\n",
              " '<sos> anda <eos>',\n",
              " '<sos> hola <eos>',\n",
              " '<sos> fuera <eos>',\n",
              " '<sos> ya <eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "spa_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97931cd3",
      "metadata": {
        "id": "97931cd3"
      },
      "outputs": [],
      "source": [
        "def build_vocab(sentences):\n",
        "    words = [word for sentence in sentences for word in sentence.split()]\n",
        "    word_count = Counter(words)\n",
        "    sorted_word_counts = sorted(word_count.items(), key=lambda x:x[1], reverse=True)\n",
        "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "    return word2idx, idx2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fa8738e",
      "metadata": {
        "id": "7fa8738e"
      },
      "outputs": [],
      "source": [
        "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
        "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
        "eng_vocab_size = len(eng_word2idx)\n",
        "spa_vocab_size = len(spa_word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79d6b633",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79d6b633",
        "outputId": "2f6c6824-0e9c-44e0-f8d9-02633f2836d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26559 45162\n"
          ]
        }
      ],
      "source": [
        "print(eng_vocab_size, spa_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e564017c",
      "metadata": {
        "id": "e564017c"
      },
      "outputs": [],
      "source": [
        "class EngSpaDataset(Dataset):\n",
        "    def __init__(self, eng_sentences, spa_sentences, eng_word2idx, spa_word2idx):\n",
        "        self.eng_sentences = eng_sentences\n",
        "        self.spa_sentences = spa_sentences\n",
        "        self.eng_word2idx = eng_word2idx\n",
        "        self.spa_word2idx = spa_word2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.eng_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        eng_sentence = self.eng_sentences[idx]\n",
        "        spa_sentence = self.spa_sentences[idx]\n",
        "        # return tokens idxs\n",
        "        eng_idxs = [self.eng_word2idx.get(word, self.eng_word2idx['<unk>']) for word in eng_sentence.split()]\n",
        "        spa_idxs = [self.spa_word2idx.get(word, self.spa_word2idx['<unk>']) for word in spa_sentence.split()]\n",
        "\n",
        "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b579577b",
      "metadata": {
        "id": "b579577b"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    eng_batch, spa_batch = zip(*batch)\n",
        "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
        "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
        "    eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=0)\n",
        "    spa_batch = torch.nn.utils.rnn.pad_sequence(spa_batch, batch_first=True, padding_value=0)\n",
        "    return eng_batch, spa_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d514b7c",
      "metadata": {
        "id": "8d514b7c"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, loss_function, optimiser, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
        "            eng_batch = eng_batch.to(device)\n",
        "            spa_batch = spa_batch.to(device)\n",
        "            # Decoder preprocessing\n",
        "            target_input = spa_batch[:, :-1]\n",
        "            target_output = spa_batch[:, 1:].contiguous().view(-1)\n",
        "            # Zero grads\n",
        "            optimiser.zero_grad()\n",
        "            # run model\n",
        "            output = model(eng_batch, target_input)\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            # loss\\\n",
        "            loss = loss_function(output, target_output)\n",
        "            # gradient and update parameters\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss/len(dataloader)\n",
        "        print(f'Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2379ea72",
      "metadata": {
        "id": "2379ea72"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e08eef6a",
      "metadata": {
        "id": "e08eef6a"
      },
      "outputs": [],
      "source": [
        "model = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n",
        "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
        "                    max_len=MAX_SEQ_LEN, dropout=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1181a12",
      "metadata": {
        "id": "a1181a12"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14e265e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14e265e9",
        "outputId": "bdaf63bd-c223-4999-feaf-649431e552c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/10, Loss: 3.6251\n",
            "Epoch: 1/10, Loss: 2.2215\n",
            "Epoch: 2/10, Loss: 1.7152\n",
            "Epoch: 3/10, Loss: 1.3823\n",
            "Epoch: 4/10, Loss: 1.1274\n",
            "Epoch: 5/10, Loss: 0.9219\n",
            "Epoch: 6/10, Loss: 0.7532\n",
            "Epoch: 7/10, Loss: 0.6230\n",
            "Epoch: 8/10, Loss: 0.5263\n",
            "Epoch: 9/10, Loss: 0.4569\n"
          ]
        }
      ],
      "source": [
        "train(model, dataloader, loss_function, optimiser, epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d271146",
      "metadata": {
        "id": "1d271146"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50740746",
      "metadata": {
        "code_folding": [],
        "id": "50740746"
      },
      "outputs": [],
      "source": [
        "def sentence_to_indices(sentence, word2idx):\n",
        "    return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
        "\n",
        "def indices_to_sentence(indices, idx2word):\n",
        "    return ' '.join([idx2word[idx] for idx in indices if idx in idx2word and idx2word[idx] != '<pad>'])\n",
        "\n",
        "def translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
        "    model.eval()\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    input_indices = sentence_to_indices(sentence, eng_word2idx)\n",
        "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n",
        "\n",
        "    # Initialize the target tensor with <sos> token\n",
        "    tgt_indices = [spa_word2idx['<sos>']]\n",
        "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            output = model(input_tensor, tgt_tensor)\n",
        "            output = output.squeeze(0)\n",
        "            next_token = output.argmax(dim=-1)[-1].item()\n",
        "            tgt_indices.append(next_token)\n",
        "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
        "            if next_token == spa_word2idx['<eos>']:\n",
        "                break\n",
        "\n",
        "    return indices_to_sentence(tgt_indices, spa_idx2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c0db72",
      "metadata": {
        "code_folding": [
          15
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2c0db72",
        "outputId": "3854e9a6-7c14-46ba-83e2-19e0dfa8f92e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence: Hello, how are you?\n",
            "Traducción: <sos> hola que tal <eos>\n",
            "\n",
            "Input sentence: I am learning artificial intelligence.\n",
            "Traducción: <sos> estoy aprendiendo inteligencia artificial <eos>\n",
            "\n",
            "Input sentence: Artificial intelligence is great.\n",
            "Traducción: <sos> la inteligencia artificial es genial <eos>\n",
            "\n",
            "Input sentence: Good night!\n",
            "Traducción: <sos> buenas noches <eos>\n",
            "\n",
            "Input sentence: you have a nice day\n",
            "Traducción: <sos> que tengas un buen dia <eos>\n",
            "\n",
            "Input sentence: evaluating the accuracy of the translation\n",
            "Traducción: <sos> la traduccion del caso es la correcta <eos>\n",
            "\n",
            "Input sentence: How good are you at translating?\n",
            "Traducción: <sos> como te va traduciendo bien <eos>\n",
            "\n",
            "Input sentence: This is a sentence that will check if the translator is capable of handling long sentences\n",
            "Traducción: <sos> la oracion es un traductor puede revisar si la oracion de un traductor <eos>\n",
            "\n",
            "Input sentence: This is a beautiful day\n",
            "Traducción: <sos> este es un bello dia <eos>\n",
            "\n",
            "Input sentence:  The advanced machine learning course has been a great experience\n",
            "Traducción: <sos> la maquina ha estado aprendiendo un gran curso por el curso <eos>\n",
            "\n",
            "Input sentence:  I love the rainy days \n",
            "Traducción: <sos> me encanta la lluvia en los dias de lluvia <eos>\n",
            "\n",
            "Input sentence: She loves books\n",
            "Traducción: <sos> ella ama los libros <eos>\n",
            "\n",
            "Input sentence: Although it was raining, he went for a walk\n",
            "Traducción: <sos> aunque estaba lloviendo el salio a dar un paseo <eos>\n",
            "\n",
            "Input sentence: It's raining cats and dogs\n",
            "Traducción: <sos> llueve a cantaros <eos>\n",
            "\n",
            "Input sentence: The kids are playing soccer\n",
            "Traducción: <sos> los ni os estan jugando futbol <eos>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_translations(model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
        "    for sentence in sentences:\n",
        "        translation = translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len, device)\n",
        "        print(f'Input sentence: {sentence}')\n",
        "        print(f'Traducción: {translation}')\n",
        "        print()\n",
        "\n",
        "# Example sentences to test the translator\n",
        "test_sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"I am learning artificial intelligence.\",\n",
        "    \"Artificial intelligence is great.\",\n",
        "    \"Good night!\",\n",
        "    \"you have a nice day\",\n",
        "    \"evaluating the accuracy of the translation\",\n",
        "    \"How good are you at translating?\",\n",
        "    \"This is a sentence that will check if the translator is capable of handling long sentences\",\n",
        "    \"This is a beautiful day\",\n",
        "    \" The advanced machine learning course has been a great experience\",\n",
        "    \" I love the rainy days \",\n",
        "    \"She loves books\",\n",
        "    \"Although it was raining, he went for a walk\",\n",
        "    \"It's raining cats and dogs\",\n",
        "    \"The kids are playing soccer\"\n",
        "]\n",
        "\n",
        "# Assuming the model is trained and loaded\n",
        "# Set the device to 'cpu' or 'cuda' as needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Evaluate translations\n",
        "evaluate_translations(model, test_sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OztBIOIO22c2"
      },
      "id": "OztBIOIO22c2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Model's Limitations  -(Limitations in translation capacity)\n",
        "\n",
        "**Sequence Length Constraints**: The PositionalEmbedding is fixed to a max sequence length. For longer texts, it may not capture full context, reducing translation quality, especially for complex sentences.\n",
        "\n",
        "**Attention Head Limitations:** While MultiHeadAttention allows focusing on different parts of a sequence, it may struggle to capture long-term dependencies, which are crucial in accurately translating nuanced or complex sentence structures.\n",
        "\n",
        "**Simple Feed-Forward Layers:** The PositionFeedForward layers, with a basic two-layer structure, may not adequately model complex, non-linear relationships needed for high-fidelity translations.\n",
        "\n",
        "**Masking in Decoding:** Masks in the DecoderSubLayer restrict visibility during generation, which could limit the model’s ability to consider entire contexts, thus affecting translation accuracy.\n",
        "\n",
        "**Effect of LayerNorm and Dropout:** While stabilizing the model, these may also soften critical information, possibly impacting detailed or precise translations.\n",
        "\n",
        "**Limited Model Depth:** A lower num_layers count may not capture intricate language structures, especially in complex texts, limiting the model’s expressive power.\n",
        "\n",
        "**Lack of Adaptive Context Mechanisms:** Without context-adaptive mechanisms, the model may struggle to handle cultural nuances or varied language styles."
      ],
      "metadata": {
        "id": "Gu1_f8Eo2d67"
      },
      "id": "Gu1_f8Eo2d67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ceefe95",
      "metadata": {
        "id": "4ceefe95"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7e10a50",
      "metadata": {
        "id": "a7e10a50"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bffb7af9",
      "metadata": {
        "id": "bffb7af9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6321db74",
      "metadata": {
        "id": "6321db74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccce7864",
      "metadata": {
        "id": "ccce7864"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}