{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b7905f-a070-4ffe-abfc-67fbcd2adaa9",
   "metadata": {},
   "source": [
    "## TC 5033\n",
    "## Deep Learning\n",
    "## Transformers\n",
    "\n",
    "#### Activity 4: Implementing a Translator\n",
    "\n",
    "- Objective\n",
    "\n",
    "To understand the Transformer Architecture by Implementing a translator.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams. While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Follow the provided code. The code already implements a transformer from scratch as explained in one of [week's 9 videos](https://youtu.be/XefFj4rLHgU)\n",
    "\n",
    "    Since the provided code already implements a simple translator, your job for this assignment is to understand it fully, and document it using pictures, figures, and markdown cells.  You should test your translator with at least 10 sentences. The dataset used for this task was obtained from [Tatoeba, a large dataset of sentences and translations](https://tatoeba.org/en/downloads).\n",
    "  \n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Traning a translator\n",
    "    - Translating at least 10 sentences.\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f54c65",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Script to convert csv to text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02c0c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#This script requires to convert the TSV file to CSV\n",
    "# easiest way is to open it in Calc or excel and save as csv\n",
    "PATH = 'eng-spa2024.csv'\n",
    "import pandas as pd\n",
    "df = pd.read_csv(PATH, header=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d9408",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eng_spa_cols = df.iloc[:, [1, 3]]\n",
    "eng_spa_cols['length'] = eng_spa_cols.iloc[:, 0].str.len()  \n",
    "eng_spa_cols = eng_spa_cols.sort_values(by='length')  \n",
    "eng_spa_cols = eng_spa_cols.drop(columns=['length'])  \n",
    "\n",
    "output_file_path = 'eng-spa4.txt'\n",
    "eng_spa_cols.to_csv(output_file_path, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d468e9a",
   "metadata": {},
   "source": [
    "## Transformer - Attention is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5dcf681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x285ba457e10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c2cbd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6623a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103d45f",
   "metadata": {
    "code_folding": [
     30,
     94
    ]
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() \n",
    "                             * (-math.log(10000.0)/d_model))\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(self.pos_embed_matrix.shape)\n",
    "#         print(x.shape)\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 512, num_heads = 8):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "        \n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        batch_size = Q.size(0)\n",
    "        '''\n",
    "        Q, K, V -> [batch_size, seq_len, num_heads*d_k]\n",
    "        after transpose Q, K, V -> [batch_size, num_heads, seq_len, d_k]\n",
    "        '''\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        \n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "        \n",
    "    def scale_dot_product(self, Q, K, V, mask = None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "\n",
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "    \n",
    "class EncoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.droupout1 = nn.Dropout(dropout)\n",
    "        self.droupout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.droupout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.droupout2(self.ffn(x))\n",
    "        return self.norm2(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n",
    "        x = x + self.dropout2(encoder_attn)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        return self.norm3(x)\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61070162",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        # Encoder mask\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "        # Embedding and positional Encoding\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
    "        source = self.pos_embedding(source)\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        \n",
    "        # Decoder embedding and postional encoding\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        target = self.pos_embedding(target)\n",
    "        # Decoder\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        \n",
    "        return self.output_layer(output)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def mask(self, source, target):\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "        size = target.size(1)\n",
    "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()\n",
    "        target_mask = target_mask & no_mask\n",
    "        return source_mask, target_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6b2d4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40581d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_len_source = 10\n",
    "seq_len_target = 10\n",
    "batch_size = 2\n",
    "input_vocab_size = 50\n",
    "target_vocab_size = 50\n",
    "\n",
    "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n",
    "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cf689",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "\n",
    "model = Transformer(d_model, num_heads, d_ff, num_layers,\n",
    "                  input_vocab_size, target_vocab_size, \n",
    "                  max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "\n",
    "model = model.to(device)\n",
    "source = source.to(device)\n",
    "target = target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618560e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = model(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0bc69d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Expected output shape -> [batch, seq_len_target, target_vocab_size] i.e. [2, 10, 50]\n",
    "print(f'ouput.shape {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b2910",
   "metadata": {},
   "source": [
    "## Translator Eng-Spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869a7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to read data\n",
    "PATH = 'eng-spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0af1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading text file\n",
    "with open(PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "# Create a list comprehension with pair of sentences\n",
    "# Using strip to delete characters at the beginin or end of the sentence\n",
    "# Splitting both sentences by Tab\n",
    "eng_spa_pairs = [line.strip().split('\\t') for line in lines if '\\t' in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c930226f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ok!', '¡OK!'],\n",
       " ['No!', '¡No!'],\n",
       " ['No.', 'No.'],\n",
       " ['So?', '¿Y qué?'],\n",
       " ['Hi.', '¡Hola!'],\n",
       " ['Go.', 'Váyase.'],\n",
       " ['OK.', 'Bueno.'],\n",
       " ['Go!', '¡Sal!'],\n",
       " ['OK.', '¡Órale!'],\n",
       " ['So?', '¿Y?']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look to top 10 pairs of sentences\n",
    "eng_spa_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c7f4b",
   "metadata": {},
   "source": [
    "## Importance of Separating English and Spanish Sentences\n",
    "\n",
    "In the process of building a simple translator, it's crucial to separate English and Spanish sentences. By splitting the sentence pairs, we can organize our data in a way that allows us to efficiently train and evaluate our translation model.\n",
    "\n",
    "### Why This Step is Important:\n",
    "\n",
    "1. **Data Organization**: Separating the sentences ensures that we have a clear structure for our input and target languages. This makes preprocessing, tokenization, and model training more manageable.\n",
    "\n",
    "2. **Model Training**: Machine translation models require input-output pairs for learning. With separate lists, it's easier to feed the English sentences as inputs and the Spanish sentences as targets to the model.\n",
    "\n",
    "3. **Preprocessing Flexibility**: Having distinct lists allows us to apply different preprocessing techniques (like tokenization or lowercasing) tailored to each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095f4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate english and spanish sentences\n",
    "eng_sentences = [pair[0] for pair in eng_spa_pairs]\n",
    "spa_sentences = [pair[1] for pair in eng_spa_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9e1c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ok!', 'No!', 'No.', 'So?', 'Hi.', 'Go.', 'OK.', 'Go!', 'OK.', 'So?']\n",
      "['¡OK!', '¡No!', 'No.', '¿Y qué?', '¡Hola!', 'Váyase.', 'Bueno.', '¡Sal!', '¡Órale!', '¿Y?']\n"
     ]
    }
   ],
   "source": [
    "# Printing some examples\n",
    "print(eng_sentences[:10])\n",
    "print(spa_sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad9514",
   "metadata": {},
   "source": [
    "## Function to preprocess sentences\n",
    "The `preprocess_sentence` function is designed to prepare text data for machine learning models, specifically in the context of natural language processing (NLP) tasks like machine translation. The function performs a series of text cleaning and formatting steps to standardize input sentences.\n",
    "\n",
    "### Why This Function is Necessary\n",
    "\n",
    "1. **Uniformity:** Standardizing the text (e.g., lowercasing and removing accents) helps the model learn more effectively by reducing variability in the input data.\n",
    "\n",
    "2. **Vocabulary Reduction:** By removing non-essential characters and normalizing text, the overall vocabulary size decreases, making the model more efficient.\n",
    "\n",
    "3. **Noise Reduction:** Eliminating punctuation and non-alphabetical characters reduces noise in the data, improving model performance.\n",
    "\n",
    "4. **Sentence Boundaries:** Adding <sos> and <eos> tokens defines clear sentence boundaries, aiding the model in understanding the structure of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d11478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Preprocesses a given sentence by cleaning and standardizing it for natural language processing (NLP) tasks.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed sentence, formatted in lowercase, stripped of accents, cleaned of non-alphabetical characters, \n",
    "             and enclosed within start and end tokens.\n",
    "\n",
    "    Steps:\n",
    "        1. Converts the sentence to lowercase and strips any leading or trailing whitespace.\n",
    "        2. Replaces multiple consecutive spaces with a single space to ensure consistent spacing.\n",
    "        3. Removes Spanish accents from vowels:\n",
    "            - Replaces accented 'á' with 'a'\n",
    "            - Replaces accented 'é' with 'e'\n",
    "            - Replaces accented 'í' with 'i'\n",
    "            - Replaces accented 'ó' with 'o'\n",
    "            - Replaces accented 'ú' with 'u'\n",
    "        4. Removes all non-alphabetical characters, including punctuation and numbers, leaving only lowercase letters.\n",
    "        5. Strips any remaining extra spaces.\n",
    "        6. Adds start (`<sos>`) and end (`<eos>`) tokens to mark the boundaries of the sentence, which is useful for models \n",
    "           that need to recognize sentence beginnings and endings.\n",
    "\n",
    "    Example:\n",
    "        input_sentence = \"¡Hola, cómo estás?\"\n",
    "        preprocessed_sentence = preprocess_sentence(input_sentence)\n",
    "        \n",
    "        Output: '<sos> hola como estas <eos>'\n",
    "    \"\"\"\n",
    "\n",
    "    # Converts all sentence to Lower Case, then stripping whitespaces at the beggining or at the end.\n",
    "    sentence = sentence.lower().strip()\n",
    "    # Replacing multiple spaces with a single space\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # Remove Spanish accent to vowel a\n",
    "    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n",
    "    # Remove Spanish accent to vowel e\n",
    "    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n",
    "    # Remove Spanish accent to vowel i\n",
    "    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n",
    "    # Remove Spanish accent to vowel o\n",
    "    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n",
    "    # Remove Spanish accent to vowel u\n",
    "    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n",
    "    # Remove all non alphabetical characters \n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
    "    # Final white spaces strip\n",
    "    sentence = sentence.strip()\n",
    "    # Adding start and end of sentence boundaries\n",
    "    sentence = '<sos> ' + sentence + ' <eos>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478f673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "s1 = '¿Hola @ cómo estás? 123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ac79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Hola @ cómo estás? 123\n",
      "<sos> hola como estas <eos>\n"
     ]
    }
   ],
   "source": [
    "# Printing original and processed sentences\n",
    "print(s1)\n",
    "print(preprocess_sentence(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9fc9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing all sentences for both languages\n",
    "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
    "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7a3b18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> ok <eos>',\n",
       " '<sos> no <eos>',\n",
       " '<sos> no <eos>',\n",
       " '<sos> y que <eos>',\n",
       " '<sos> hola <eos>',\n",
       " '<sos> vayase <eos>',\n",
       " '<sos> bueno <eos>',\n",
       " '<sos> sal <eos>',\n",
       " '<sos> orale <eos>',\n",
       " '<sos> y <eos>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing some examples\n",
    "spa_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194548a",
   "metadata": {},
   "source": [
    "## DEFINNING VOCABULARY\n",
    "The `build_vocab` function creates two dictionaries, `word2idx` and `idx2word`, which are essential components for mapping words to unique integer indices and vice versa. These mappings are crucial for processing text data in NLP tasks, as most machine learning models require numerical input rather than raw text.\n",
    "\n",
    "### Importance of The Function\n",
    "1. **Numerical Representation:** Machine learning models require numerical input. The word2idx dictionary allows us to convert words into integer indices that the model can process.\n",
    "\n",
    "2. **Efficient Handling of Unknown Words:** By including the <unk> token, the function ensures that words not seen during vocabulary building can still be represented, preventing errors during model inference.\n",
    "\n",
    "3. **Sequence Padding:** The <pad> token is essential for making all sequences in a batch the same length, which is necessary for efficient batch processing in NLP models.\n",
    "Frequency-Based Vocabulary: Sorting words by frequency ensures that the most common words are prioritized, which can improve model performance and reduce the impact of rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97931cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates two dictionaries with the unique words in the vocabulary\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from a list of sentences, creating mappings from words to unique integer indices and vice versa.\n",
    "\n",
    "    Args:\n",
    "        sentences (list of str): A list of sentences, where each sentence is a string of words separated by spaces.\n",
    "\n",
    "    Returns:\n",
    "        Two dictionaries:\n",
    "            - word2idx (dict): A mapping from words to unique indices. Includes special tokens:\n",
    "                * '<pad>' mapped to index 0, used for padding sequences.\n",
    "                * '<unk>' mapped to index 1, used for unknown or out-of-vocabulary words.\n",
    "            - idx2word (dict): A mapping from indices to words, the reverse of `word2idx`.\n",
    "\n",
    "    Steps:\n",
    "        1. Extract all words from the given sentences and compile them into a list.\n",
    "        2. Count the frequency of each word using `Counter` from the `collections` module.\n",
    "        3. Sort the words in descending order of frequency, ensuring that the most common words have the lowest indices.\n",
    "        4. Create the `word2idx` dictionary, starting the index from 2 (reserving 0 for '<pad>' and 1 for '<unk>').\n",
    "        5. Add special tokens '<pad>' and '<unk>' to `word2idx`.\n",
    "        6. Build the `idx2word` dictionary by reversing the mappings in `word2idx`.\n",
    "    \n",
    "    Example:\n",
    "        sentences = [\"this is a sentence\", \"another sentence here\"]\n",
    "        word2idx, idx2word = build_vocab(sentences)\n",
    "        \n",
    "        word2idx could look like:\n",
    "        {'<pad>': 0, '<unk>': 1, 'sentence': 2, 'this': 3, 'is': 4, 'a': 5, 'another': 6, 'here': 7}\n",
    "        \n",
    "        idx2word could look like:\n",
    "        {0: '<pad>', 1: '<unk>', 2: 'sentence', 3: 'this', 4: 'is', 5: 'a', 6: 'another', 7: 'here'}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating list that contains all different words\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    # Counting frequency of words\n",
    "    word_count = Counter(words)\n",
    "    # Sorting words in descending order, so the first indexes will be the most repeated ones.\n",
    "    sorted_word_counts = sorted(word_count.items(), key=lambda x:x[1], reverse=True)\n",
    "    # Creating dictionary that allows to get index from a word\n",
    "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
    "    # Definning index 0 and one for padding and unknown.\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "    # Creating dictionary that allows to get word from an index\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fa8738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionaries for english sentences\n",
    "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
    "# Creating dictionaries for spanish sentences\n",
    "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
    "# Saving size of language vocabulary into variables for each language\n",
    "eng_vocab_size = len(eng_word2idx)\n",
    "spa_vocab_size = len(spa_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79d6b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27653, 46934\n"
     ]
    }
   ],
   "source": [
    "# Printing vocabulary size for each language\n",
    "print(f'{eng_vocab_size}, {spa_vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291589d",
   "metadata": {},
   "source": [
    "## English - Spanish Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e564017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngSpaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch dataset class for handling English-Spanish sentence pairs for machine translation tasks.\n",
    "    It prepares the dataset by converting sentences into indexed sequences that can be used for model training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eng_sentences, spa_sentences, eng_word2idx, spa_word2idx):\n",
    "        \"\"\"\n",
    "        Initializes the EngSpaDataset with English and Spanish sentences, and word-to-index mappings.\n",
    "\n",
    "        Args:\n",
    "            eng_sentences (list of str): A list of English sentences.\n",
    "            spa_sentences (list of str): A list of Spanish sentences.\n",
    "            eng_word2idx (dict): A dictionary that maps English words to indices.\n",
    "            spa_word2idx (dict): A dictionary that maps Spanish words to indices.\n",
    "        \"\"\"\n",
    "\n",
    "        self.eng_sentences = eng_sentences\n",
    "        self.spa_sentences = spa_sentences\n",
    "        self.eng_word2idx = eng_word2idx\n",
    "        self.spa_word2idx = spa_word2idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of sentence pairs in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of sentence pairs in the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.eng_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a specific sentence pair (English and Spanish) from the dataset and converts it into a tensor of indices.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sentence pair to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two tensors:\n",
    "                - The first tensor is the English sentence converted to a sequence of word indices.\n",
    "                - The second tensor is the Spanish sentence converted to a sequence of word indices.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        spa_sentence = self.spa_sentences[idx]\n",
    "\n",
    "        # Convert English and Spanish sentences to word indices\n",
    "        eng_idxs = [self.eng_word2idx.get(word, self.eng_word2idx['<unk>']) for word in eng_sentence.split()]\n",
    "        spa_idxs = [self.spa_word2idx.get(word, self.spa_word2idx['<unk>']) for word in spa_sentence.split()]\n",
    "        \n",
    "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c170fb",
   "metadata": {},
   "source": [
    "## Collate\n",
    "\n",
    "The `collate_fn` function is a custom collate function for batching and padding sequences in a machine translation task. It is designed to be used with PyTorch's `DataLoader` to prepare batches of variable-length sentences for training a model. This function ensures that each batch of English and Spanish sentences is uniformly padded, making it compatible with the model's input requirements.\n",
    "\n",
    "### Importance of the collate_fn Function\n",
    "\n",
    "The collate_fn function is crucial for handling variable-length sentences in a machine translation task. Here's why it is important:\n",
    "\n",
    "1. **Uniform Sequence Lengths:** In NLP tasks, sentences often have different lengths. This function ensures that all sequences in a batch are the same length by padding shorter sequences. This uniformity is essential for efficient batch processing in neural networks.\n",
    "\n",
    "2. **Efficient Model Training:** By truncating sequences to a maximum length, the function reduces the computational burden, making the training process faster and more memory-efficient. This step is particularly important when dealing with large datasets.\n",
    "\n",
    "3. **Ease of Use with DataLoader:** The collate_fn function seamlessly integrates with PyTorch's DataLoader, allowing for efficient batching, padding, and loading of data. This integration simplifies the data preparation pipeline for model training.\n",
    "\n",
    "4. **Handling Padding:** The use of a specific padding value (0 in this case) ensures that the model can easily differentiate between meaningful tokens and padding tokens, which is important for sequence models that need to ignore padding during computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    A custom collate function for batching and padding sequences in a machine translation task.\n",
    "    This function is designed to work with PyTorch's DataLoader, ensuring that all sequences in a batch \n",
    "    have the same length by padding them as needed.\n",
    "\n",
    "    Args:\n",
    "        batch (list of tuples): A batch of sentence pairs, where each tuple contains:\n",
    "            - eng_batch (torch.Tensor): The English sentence as a tensor of word indices.\n",
    "            - spa_batch (torch.Tensor): The Spanish sentence as a tensor of word indices.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two padded tensors:\n",
    "            - eng_batch (torch.Tensor): A tensor containing the padded English sentences.\n",
    "            - spa_batch (torch.Tensor): A tensor containing the padded Spanish sentences.\n",
    "\n",
    "    Steps:\n",
    "        1. Receives a batch of English and Spanish sentences (in tensor format) and separates them using `zip`.\n",
    "        2. Truncates each sentence to a maximum sequence length (`MAX_SEQ_LEN`), if necessary, to prevent overly long sequences.\n",
    "        3. Uses PyTorch's `pad_sequence` utility to pad all English and Spanish sequences to the same length.\n",
    "           - `padding_value=0` is used to pad the sequences, which corresponds to the `<pad>` token.\n",
    "        4. Returns the padded tensors, making them ready for input to a model.\n",
    "\n",
    "    Example:\n",
    "        batch = [(torch.tensor([2, 5, 6]), torch.tensor([3, 8, 1, 7])),\n",
    "                 (torch.tensor([4, 6]), torch.tensor([9, 2]))]\n",
    "        eng_batch, spa_batch = collate_fn(batch)\n",
    "        \n",
    "        # eng_batch could look like:\n",
    "        # tensor([[2, 5, 6],\n",
    "        #         [4, 6, 0]])\n",
    "        \n",
    "        # spa_batch could look like:\n",
    "        # tensor([[3, 8, 1, 7],\n",
    "        #         [9, 2, 0, 0]])\n",
    "    \"\"\"\n",
    "\n",
    "    # Receiving batch of english and spanish batch\n",
    "    eng_batch, spa_batch = zip(*batch)\n",
    "    # We are going to truncate sentences to Max sequence lenght\n",
    "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
    "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
    "    # Pad the sequences to the same length\n",
    "    eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=0)\n",
    "    spa_batch = torch.nn.utils.rnn.pad_sequence(spa_batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return eng_batch, spa_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317a849",
   "metadata": {},
   "source": [
    "## Train Function\n",
    "\n",
    "### Description\n",
    "The `train` function is a key component of the machine translation pipeline. It trains a sequence-to-sequence model using a given dataset and optimiser for a specified number of epochs. This function performs the essential tasks of forward propagation, loss computation, backpropagation, and model parameter updates.\n",
    "\n",
    "### Importance of the train Function\n",
    "\n",
    "1. **Model Optimization:** The train function is responsible for optimizing the model's parameters, making it capable of accurately translating sentences from English to Spanish.\n",
    "2. **Efficient Data Handling:** By using PyTorch's DataLoader and moving data to the GPU, the function ensures efficient data processing, which is crucial for training large models on big datasets.\n",
    "3. **Loss Monitoring:** The function computes and prints the average loss for each epoch, providing insight into how well the model is learning and converging.\n",
    "4. **Gradient Descent Implementation:** It applies the core concept of gradient descent, updating the model's weights to minimize the loss, which is fundamental for training deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d514b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_function, optimiser, epochs):\n",
    "    \"\"\"\n",
    "    Trains a sequence-to-sequence model for a specified number of epochs using a given dataset and optimiser.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The machine translation model to be trained.\n",
    "        dataloader (DataLoader): A PyTorch DataLoader object that provides batches of English and Spanish sentences.\n",
    "        loss_function (nn.Module): The loss function used to compute the difference between the predicted and actual outputs.\n",
    "        optimiser (torch.optim.Optimizer): The optimiser used to update the model parameters.\n",
    "        epochs (int): The number of times the training loop should iterate over the dataset.\n",
    "\n",
    "    Steps:\n",
    "        1. Sets the model to training mode using `model.train()`.\n",
    "        2. Loops over the dataset for the specified number of epochs.\n",
    "        3. Iterates over each batch of English and Spanish sentence pairs in the dataloader.\n",
    "        4. Moves the batches to the GPU if available, using `device`.\n",
    "        5. Prepares the decoder inputs and targets by splitting the Spanish sentence tensors.\n",
    "        6. Zeroes out the gradients of the optimiser to prevent accumulation from previous iterations.\n",
    "        7. Passes the English and decoder input sequences through the model to generate outputs.\n",
    "        8. Reshapes the model's output and computes the loss using the loss function.\n",
    "        9. Performs backpropagation to compute the gradients and updates the model parameters using the optimiser.\n",
    "        10. Accumulates the loss and prints the average loss at the end of each epoch.\n",
    "\n",
    "    Example:\n",
    "        train(model, dataloader, loss_function, optimiser, epochs=10)\n",
    "    \"\"\"\n",
    "\n",
    "    # Set model into trainning mode\n",
    "    model.train()\n",
    "    # Trainning loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0 \n",
    "        # Loop to go through each minibatch\n",
    "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
    "            # Sending batch to GPU (if available)\n",
    "            eng_batch = eng_batch.to(device)\n",
    "            spa_batch = spa_batch.to(device)\n",
    "            # Decoder preprocessing\n",
    "            target_input = spa_batch[:, :-1]\n",
    "            target_output = spa_batch[:, 1:].contiguous().view(-1)\n",
    "            # Zero grads\n",
    "            optimiser.zero_grad()\n",
    "            # run model\n",
    "            output = model(eng_batch, target_input)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            # calculating lossfunction\n",
    "            loss = loss_function(output, target_output)\n",
    "            # gradient and update parameters\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss/len(dataloader)\n",
    "\n",
    "        print(f'Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Batch Size\n",
    "BATCH_SIZE = 64\n",
    "# Creating Dataset\n",
    "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
    "# Creating DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08eef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model\n",
    "model = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n",
    "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1181a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending model to GPU (if available)\n",
    "model = model.to(device)\n",
    "# Defining loss function, ignoring index to not take into consideration padding characters\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "# Defining optimiser\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e265e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainning the model\n",
    "train(model, dataloader, loss_function, optimiser, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d271146",
   "metadata": {},
   "source": [
    "## Function to test the model\n",
    "\n",
    "### Description\n",
    "The `translate_sentence` function is a critical part of the translation pipeline. It takes an English sentence, processes it, and uses a trained model to generate a translated Spanish sentence. The function utilizes helper functions to convert sentences to and from sequences of word indices, facilitating model input and output handling.\n",
    "\n",
    "### Importance of the translate_sentence Function\n",
    "\n",
    "1. **Practical Translation:** This function transforms English text into Spanish, making the model usable for real-world applications.\n",
    "2. **Efficient Evaluation:** The use of torch.no_grad() ensures that the translation process is efficient by disabling gradient calculations.\n",
    "3. **Dynamic Word Generation:** By generating words one at a time, the function can handle sentences of varying lengths and complexity, adapting to different input scenarios.\n",
    "4. **Model Evaluation Mode:** Setting the model to evaluation mode ensures that the model behaves appropriately during inference, improving reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50740746",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence, word2idx):\n",
    "    \"\"\"\n",
    "    Converts a sentence into a list of word indices based on a given word-to-index dictionary.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The input sentence to be converted.\n",
    "        word2idx (dict): A dictionary mapping words to their corresponding indices.\n",
    "    \n",
    "    Returns:\n",
    "        List[int]: A list of word indices, using the index for '<unk>' for words not in the dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
    "\n",
    "def indices_to_sentence(indices, idx2word):\n",
    "    \"\"\"\n",
    "    Converts a list of word indices back into a readable sentence.\n",
    "    \n",
    "    Args:\n",
    "        indices (List[int]): A list of word indices.\n",
    "        idx2word (dict): A dictionary mapping indices to their corresponding words.\n",
    "    \n",
    "    Returns:\n",
    "        str: A reconstructed sentence with words joined by spaces, excluding the '<pad>' token.\n",
    "    \"\"\"\n",
    "\n",
    "    return ' '.join([idx2word[idx] for idx in indices if idx in idx2word and idx2word[idx] != '<pad>'])\n",
    "\n",
    "def translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    \"\"\"\n",
    "    Translates an English sentence into Spanish using a trained model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained sequence-to-sequence model.\n",
    "        sentence (str): The English sentence to be translated.\n",
    "        eng_word2idx (dict): A dictionary mapping English words to indices.\n",
    "        spa_idx2word (dict): A dictionary mapping Spanish indices to words.\n",
    "        max_len (int): The maximum length of the translated sentence. Defaults to MAX_SEQ_LEN.\n",
    "        device (str): The device ('cpu' or 'cuda') on which to run the translation. Defaults to 'cpu'.\n",
    "    \n",
    "    Returns:\n",
    "        str: The translated Spanish sentence.\n",
    "    \n",
    "    Steps:\n",
    "        1. Sets the model to evaluation mode.\n",
    "        2. Preprocesses the input English sentence.\n",
    "        3. Converts the preprocessed sentence into a tensor of word indices.\n",
    "        4. Initializes the target sequence with the '<sos>' token.\n",
    "        5. Runs a loop to generate the translation word by word, up to `max_len` words.\n",
    "        6. Appends the predicted word to the target sequence until the '<eos>' token is produced or `max_len` is reached.\n",
    "        7. Converts the sequence of indices into a readable Spanish sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess the sentence and convert it to indices\n",
    "    model.eval()\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    input_indices = sentence_to_indices(sentence, eng_word2idx)\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    # Initialize the target tensor with <sos> token\n",
    "    tgt_indices = [spa_word2idx['<sos>']]\n",
    "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_tensor, tgt_tensor)\n",
    "            output = output.squeeze(0)\n",
    "            next_token = output.argmax(dim=-1)[-1].item()\n",
    "            tgt_indices.append(next_token)\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "            if next_token == spa_word2idx['<eos>']:\n",
    "                break\n",
    "    \n",
    "    # Convert the list of indices into a readable sentence\n",
    "    return indices_to_sentence(tgt_indices, spa_idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0db72",
   "metadata": {
    "code_folding": [
     15
    ]
   },
   "outputs": [],
   "source": [
    "# Evaluating results of some sentences\n",
    "def evaluate_translations(model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    for sentence in sentences:\n",
    "        translation = translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len, device)\n",
    "        print(f'Input sentence: {sentence}')\n",
    "        print(f'Traducción: {translation}')\n",
    "        print()\n",
    "\n",
    "# Example sentences to test the translator\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am learning artificial intelligence.\",\n",
    "    \"Artificial intelligence is great.\",\n",
    "    \"Good night!\"\n",
    "]\n",
    "\n",
    "# Assuming the model is trained and loaded\n",
    "# Set the device to 'cpu' or 'cuda' as needed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Evaluate translations\n",
    "evaluate_translations(model, test_sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
