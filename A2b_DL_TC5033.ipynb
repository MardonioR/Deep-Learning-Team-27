{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Convolutional Neural Networks\n",
    "<br>\n",
    "\n",
    "#### Activity 2b: Building a CNN for CIFAR10 dataset with PyTorch\n",
    "<br>\n",
    "\n",
    "- Objective\n",
    "\n",
    "    The main goal of this activity is to further your understanding of Convolutional Neural Networks (CNNs) by building one using PyTorch. You will apply this architecture to the famous CIFAR10 dataset, taking what you've learned from the guide code that replicated the Fully Connected model in PyTorch (Activity 2a).\n",
    "\n",
    "- Instructions\n",
    "    This activity requires submission in teams of 5 or 6 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Understand the Guide Code: Review the guide code from Activity 2a that implemented a Fully Connected model in PyTorch. Note how PyTorch makes it easier to implement neural networks.\n",
    "\n",
    "    Familiarize Yourself with CNNs: Take some time to understand their architecture and the rationale behind using convolutional layers.\n",
    "\n",
    "    Prepare the Dataset: Use PyTorch's DataLoader to manage the dataset. Make sure the data is appropriately preprocessed for a CNN.\n",
    "\n",
    "    Design the CNN Architecture: Create a new architecture that incorporates convolutional layers. Use PyTorch modules like nn.Conv2d, nn.MaxPool2d, and others to build your network.\n",
    "\n",
    "    Training Loop and Backpropagation: Implement the training loop, leveraging PyTorch’s autograd for backpropagation. Keep track of relevant performance metrics.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to explain your architectural decisions, performance results, and any challenges you faced. Compare this model with your previous Fully Connected model in terms of performance and efficiency.\n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Understanding of CNN architecture and its application to the CIFAR10 dataset\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness and efficiency of the chosen CNN architecture\n",
    "    - Correct implementation of Traning Loop and Accuracy Function\n",
    "    - Model's performance metrics on the CIFAR10 dataset (at least 65% accuracy)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit via Canvas your Jupyter Notebook with the CNN implemented in PyTorch. Your submission should include well-commented code and Markdown cells that provide a comprehensive view of your design decisions, performance metrics, and learnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'CIFAR10/'\n",
    "NUM_TRAIN = 50000\n",
    "NUM_VAL = 5000\n",
    "NUM_TEST = 5000\n",
    "MINIBATCH_SIZE = 64\n",
    "\n",
    "transform_cifar = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.491, 0.482, 0.447], [0.247, 0.243, 0.261])\n",
    "            ])\n",
    "\n",
    "# Train dataset\n",
    "cifar10_train = datasets.CIFAR10(DATA_PATH, train=True, download=True,\n",
    "                             transform=transform_cifar)\n",
    "train_loader = DataLoader(cifar10_train, batch_size=MINIBATCH_SIZE, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "#Validation set\n",
    "cifar10_val = datasets.CIFAR10(DATA_PATH, train=False, download=True,\n",
    "                           transform=transform_cifar)\n",
    "val_loader = DataLoader(cifar10_val, batch_size=MINIBATCH_SIZE, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_VAL)))\n",
    "#Test set\n",
    "cifar10_test = datasets.CIFAR10(DATA_PATH, train=False, download=True, \n",
    "                            transform=transform_cifar)\n",
    "test_loader = DataLoader(cifar10_test, batch_size=MINIBATCH_SIZE,\n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_VAL, len(cifar10_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the object\n",
    "cifar10_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Batch size\n",
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing data from data loader\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    print(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using  GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting cuada as device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show images\n",
    "classes = test_loader.dataset.classes\n",
    "def plot_figure(image):\n",
    "    plt.imshow(np.transpose(image,(1,2,0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "rnd_sample_idx = np.random.randint(len(test_loader))\n",
    "print(f'La imagen muestreada representa un: {classes[test_loader.dataset[rnd_sample_idx][1]]}')\n",
    "image = test_loader.dataset[rnd_sample_idx][0]\n",
    "image = (image - image.min()) / (image.max() -image.min() )\n",
    "plot_figure(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly show 8 images by class in a grid\n",
    "def plot_cifar10_grid():\n",
    "    classes = test_loader.dataset.classes\n",
    "    total_samples = 8\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for label, sample in enumerate(classes):\n",
    "        class_idxs = np.flatnonzero(label == np.array(test_loader.dataset.targets))\n",
    "        sample_idxs = np.random.choice(class_idxs, total_samples, replace = False)\n",
    "        for i, idx in enumerate(sample_idxs):\n",
    "            plt_idx = i*len(classes) + label + 1\n",
    "            plt.subplot(total_samples, len(classes), plt_idx)\n",
    "            plt.imshow(test_loader.dataset.data[idx])\n",
    "            plt.axis('off')\n",
    "            \n",
    "            if i == 0: plt.title(sample)\n",
    "    plt.show()\n",
    "\n",
    "plot_cifar10_grid() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def accuracy(model, loader):\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    model.eval()    # Ponemos el modelo en modo evaluación\n",
    "    model = model.to(device=device) # Nos aseguramos que el modelo esté en GPUs\n",
    "    with torch.no_grad():\n",
    "        for xi, yi in loader:\n",
    "            xi = xi.to(device=device, dtype = torch.float32)\n",
    "            yi = yi.to(device=device, dtype = torch.long)\n",
    "            scores = model(xi) \n",
    "            _, pred = scores.max(dim=1) \n",
    "            num_correct += (pred == yi).sum() # Comparamos la predicción con los elementos de la clase correcta\n",
    "            num_total += pred.size(0)\n",
    "        return float(num_correct)/num_total   # Elementos predichos entre total de elementos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento\n",
    "def train(model, optimiser, epochs=100):\n",
    "    model = model.to(device=device)\n",
    "    for epoch in range(epochs):\n",
    "        for i, (xi, yi) in enumerate(train_loader):\n",
    "            model.train()   # Ponemos modelo en modo entrenamiento\n",
    "            xi = xi.to(device=device, dtype=torch.float32)\n",
    "            yi = yi.to(device=device, dtype=torch.long)\n",
    "            scores = model(xi)\n",
    "            cost = F.cross_entropy(input= scores, target=yi)\n",
    "            optimiser.zero_grad()           \n",
    "            cost.backward() # Llamamos a la función backward\n",
    "            optimiser.step()              \n",
    "        acc = accuracy(model, val_loader) \n",
    "        print(f'Epoch: {epoch+1}, costo: {cost.item()}, accuracy: {acc},')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### **Evaluación 1**  \n",
    "- **Modelo**: `Linear NN`\n",
    "- **Hiperparámetros**:  \n",
    "  - `Learning Rate`: `0.001`  \n",
    "  - `Epochs`: `10`\n",
    "  - `Hidden Layer 1`: `256`\n",
    "  - `Hidden Layer 2`: `256`\n",
    "  \n",
    "- **Resultados**:  \n",
    "  - **Train Accuracy**: **`52.98%`** \n",
    "  - **Test Accuracy**: **`51.48%`**  \n",
    "---\n",
    "  \n",
    "##### **Evaluación 2**  \n",
    "- **Modelo**: `CNN Básica (con Data Augmentation)`\n",
    "- **Hiperparámetros**:  \n",
    "  - `Learning Rate`: `0.00055`  \n",
    "  - `Epochs`: `25`\n",
    "  - `Hidden Layer 1`: `512`\n",
    "  - `Hidden Layer 2`: `256`\n",
    "  - `Hidden Layer 2`: `128`\n",
    "\n",
    "- **Resultados**:  \n",
    "  - **Train Accuracy**: **`53.04%`**  \n",
    "  - **Test Accuracy**: **`53.4%`**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros\n",
    "hidden2 = 512 \n",
    "hidden1 = 256 \n",
    "hidden = 128\n",
    "lr = 0.00055\n",
    "epochs = 25\n",
    "# Creando red neuronal linear\n",
    "model1 = nn.Sequential(nn.Flatten(),\n",
    "                       nn.Linear(in_features=32*32*3, out_features=hidden2), nn.ReLU(),\n",
    "                       nn.Linear(in_features=hidden2, out_features=hidden1), nn.ReLU(),\n",
    "                       nn.Linear(in_features=hidden1, out_features=hidden), nn.ReLU(),\n",
    "                       nn.Linear(in_features=hidden, out_features=10))  # 10 elementos de salida pues tenemos 10 clases\n",
    "optimiser = torch.optim.Adam(model1.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenando modelo\n",
    "train(model1, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el accuracy con los datos de prueba\n",
    "accuracy(model1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQUENTIAL CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### **Evaluación 1**  \n",
    "- **Modelo**: `Convolutional NN`\n",
    "- **Hiperparámetros**:  \n",
    "  - `Learning Rate`: `0.0001`  \n",
    "  - `Epochs`: `30`\n",
    "  - `Convolutional Layer 1`: `16`\n",
    "  - `Convolutional Layer 2`: `32`\n",
    "  - `Max Pooling`: `2x2`\n",
    "  \n",
    "- **Resultados**:  \n",
    "  - **Train Accuracy**: **`66.70%`**  \n",
    "  - **Test Accuracy**: **`65.14%`**  \n",
    "---\n",
    "  \n",
    "##### **Evaluación 2**  \n",
    "- **Modelo**: `Convolutional NN`\n",
    "- **Hiperparámetros**:  \n",
    "  - `Learning Rate`: `0.0001`  \n",
    "  - `Epochs`: `20`\n",
    "  - `Convolutional Layer 1`: `64`\n",
    "  - `Convolutional Layer 2`: `32`\n",
    "  - `Max Pooling`: `2x2`\n",
    "\n",
    "- **Resultados**:  \n",
    "  - **Train Accuracy**: **`68.44%`**  \n",
    "  - **Test Accuracy**: **`67.92%`**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolusional con kernel = 3 y padding = 1\n",
    "# Creamos esta función para las redes convolusionales\n",
    "conv_k_3 = lambda channel1, channel2 : nn.Conv2d(channel1,channel2, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiendo clase para hacer la red neuronal convolusional con 2 capas y max pooling.\n",
    "class CNN_class1(nn.Module):\n",
    "    def __init__(self, in_channel, channel1, channel2):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_k_3(in_channel,channel1)\n",
    "        self.conv2 = conv_k_3(channel1,channel2)\n",
    "        self.max_pool = nn.MaxPool2d(2,2)\n",
    "        self.fc = nn.Linear(in_features=16*16*channel2, out_features=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv2(F.relu(self.conv1(x))))\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel1 = 64   # Numero de filtros en el primer layer\n",
    "channel2 = 32   # Numero de filtros en la segunda layer\n",
    "epochs = 20\n",
    "lr = 0.0001\n",
    "modelCNN1 = CNN_class1(3, channel1, channel2)\n",
    "optimiser = torch.optim.Adam(modelCNN1.parameters(), lr)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "train(modelCNN1, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el accuracy con los datos de prueba\n",
    "accuracy(modelCNN1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN CON BATCH NORMALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### **Evaluación 1**  \n",
    "- **Modelo**: `Convolutional NN`\n",
    "- **Hiperparámetros**:  \n",
    "  - `Learning Rate`: `0.0001`  \n",
    "  - `Epochs`: `25`\n",
    "  - `Convolutional Layer 1`: `32`\n",
    "  - `Batch Normalization`\n",
    "  - `Convolutional Layer 2`: `64`\n",
    "  - `Batch Normalization`\n",
    "  - `Max Pooling`: `2x2`\n",
    "  - `Convolutional Layer 3`: `128`\n",
    "  - `Batch Normalization`\n",
    "  - `Convolutional Layer 4`: `256`\n",
    "  - `Batch Normalization`\n",
    "  - `Max Pooling`: `2x2`\n",
    "\n",
    "- **Resultados**:  \n",
    "  - **Train Accuracy**: **`71.14%`**  \n",
    "  - **Test Accuracy**: **`70.94%`**  \n",
    "---\n",
    "  \n",
    "##### **Evaluación 2**  \n",
    "- **Modelo**: `Convolutional NN`\n",
    "- **Hiperparámetros**:  \n",
    "  - `Learning Rate`: `0.0001`  \n",
    "  - `Epochs`: `25`\n",
    "  - `Convolutional Layer 1`: `16`\n",
    "  - `Batch Normalization`\n",
    "  - `Convolutional Layer 2`: `32`\n",
    "  - `Batch Normalization`\n",
    "  - `Max Pooling`: `2x2`\n",
    "  - `Convolutional Layer 3`: `64`\n",
    "  - `Batch Normalization`\n",
    "  - `Convolutional Layer 4`: `128`\n",
    "  - `Batch Normalization`\n",
    "  - `Max Pooling`: `2x2`\n",
    "\n",
    "- **Resultados**:  \n",
    "  - **Train Accuracy**: **`73.62%`**  \n",
    "  - **Test Accuracy**: **`72.66%`**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiendo clases para hacer la red neuronal convolusional con dos capas y batch normalization después de cada capa\n",
    "# Se agrega también Max pooling al final de estas\n",
    "class CNN_class2(nn.Module):\n",
    "    def __init__(self, in_channel, channel1, channel2, channel3, channel4):\n",
    "        super().__init__()\n",
    "        # Primer Capa\n",
    "        self.conv1 = conv_k_3(in_channel,channel1)\n",
    "        self.bn1 = nn.BatchNorm2d(channel1)\n",
    "\n",
    "        # Segunda Capa\n",
    "        self.conv2 = conv_k_3(channel1,channel2)\n",
    "        self.bn2 = nn.BatchNorm2d(channel2)\n",
    "\n",
    "        # Tercera Capa\n",
    "        self.conv3 = conv_k_3(channel2,channel3)\n",
    "        self.bn3 = nn.BatchNorm2d(channel3)\n",
    "\n",
    "        # Cuarta Capa\n",
    "        self.conv4 = conv_k_3(channel3,channel4)\n",
    "        self.bn4 = nn.BatchNorm2d(channel4)\n",
    "\n",
    "        # Max Pooling\n",
    "        self.max_pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=8*8*channel4, out_features=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Aplicamos la primer capa convolusional, después hacemos batch normalization y finalmente su función de activación RELU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Aplicamos la segunda capa convolusional, después hacemos batch normalization y finalmente su finción de activación RELU\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        # Aplicamos Max Pooling después de este primer par de capas\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        # Aplicamos la tercer capa convolusional, después hacemos batch normalization y finalmente su función de activación RELU\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # Aplicamos la cuarta capa convolusional, después hacemos batch normalization y finalmente su finción de activación RELU\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        # Aplicamos Max Pooling después de este segundo par de capas\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, costo: 1.0900850296020508, accuracy: 0.6068,\n",
      "Epoch: 2, costo: 1.2050460577011108, accuracy: 0.6656,\n",
      "Epoch: 3, costo: 0.8763821125030518, accuracy: 0.6928,\n",
      "Epoch: 4, costo: 0.8058571219444275, accuracy: 0.6904,\n",
      "Epoch: 5, costo: 0.5028868317604065, accuracy: 0.7026,\n",
      "Epoch: 6, costo: 0.32012438774108887, accuracy: 0.719,\n",
      "Epoch: 7, costo: 1.0410475730895996, accuracy: 0.7388,\n",
      "Epoch: 8, costo: 0.7200767397880554, accuracy: 0.7286,\n",
      "Epoch: 9, costo: 0.37254637479782104, accuracy: 0.734,\n",
      "Epoch: 10, costo: 0.6962599158287048, accuracy: 0.7248,\n",
      "Epoch: 11, costo: 0.416248083114624, accuracy: 0.7438,\n",
      "Epoch: 12, costo: 0.2988698184490204, accuracy: 0.7364,\n",
      "Epoch: 13, costo: 0.9376838803291321, accuracy: 0.7374,\n",
      "Epoch: 14, costo: 0.6824215650558472, accuracy: 0.7444,\n",
      "Epoch: 15, costo: 0.34041354060173035, accuracy: 0.738,\n",
      "Epoch: 16, costo: 0.5254177451133728, accuracy: 0.741,\n",
      "Epoch: 17, costo: 0.6479150056838989, accuracy: 0.744,\n",
      "Epoch: 18, costo: 0.2111598253250122, accuracy: 0.7398,\n",
      "Epoch: 19, costo: 0.32345640659332275, accuracy: 0.7322,\n",
      "Epoch: 20, costo: 0.18674740195274353, accuracy: 0.7354,\n",
      "Epoch: 21, costo: 0.10891424864530563, accuracy: 0.7264,\n",
      "Epoch: 22, costo: 0.2557014524936676, accuracy: 0.7292,\n",
      "Epoch: 23, costo: 0.045118268579244614, accuracy: 0.7364,\n",
      "Epoch: 24, costo: 0.13007402420043945, accuracy: 0.7398,\n",
      "Epoch: 25, costo: 0.04647349193692207, accuracy: 0.7362,\n"
     ]
    }
   ],
   "source": [
    "channel1 = 16   # Numero de filtros en el primer layer\n",
    "channel2 = 32   # Numero de filtros en la segunda layer\n",
    "channel3 = 64   # Numero de filtros en la tercera layer\n",
    "channel4 = 128   # Numero de filtros en la cuarta layer\n",
    "\n",
    "epochs = 25\n",
    "lr = 0.0001\n",
    "modelCNN2 = CNN_class2(3, channel1, channel2, channel3, channel4)\n",
    "optimiser = torch.optim.Adam(modelCNN2.parameters(), lr)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "train(modelCNN2, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7266"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver el accuracy con los datos de prueba\n",
    "accuracy(modelCNN2, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN CON BATCH NORMALIZATION Y DROPOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### **Evaluación 1**  \n",
    "- **Modelo**: `Convolutional NN`\n",
    "- **Hiperparámetros**:  \n",
    "  - `Learning Rate`: `0.0001`  \n",
    "  - `Epochs`: `30`\n",
    "  - `Convolutional Layer 1`: `32`\n",
    "  - `Batch Normalization`\n",
    "  - `Convolutional Layer 2`: `64`\n",
    "  - `Batch Normalization`\n",
    "  - `Max Pooling`: `2x2`\n",
    "  - `Convolutional Layer 3`: `128`\n",
    "  - `Batch Normalization`\n",
    "  - `Convolutional Layer 4`: `256`\n",
    "  - `Batch Normalization`\n",
    "  - `Max Pooling`: `2x2`\n",
    "\n",
    "  - `Dropout`: `p = 0.5`\n",
    "\n",
    "- **Resultados**:  \n",
    "  - **Train Accuracy**: **`71.14%`**  \n",
    "  - **Test Accuracy**: **`70.94%`**  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiendo clases para hacer la red neuronal convolusional con dos capas y batch normalization después de cada capa\n",
    "# Se agrega también Max pooling al final de estas\n",
    "class CNN_class2(nn.Module):\n",
    "    def __init__(self, in_channel, channel1, channel2, channel3, channel4):\n",
    "        super().__init__()\n",
    "        # Primer Capa\n",
    "        self.conv1 = conv_k_3(in_channel,channel1)\n",
    "        self.bn1 = nn.BatchNorm2d(channel1)\n",
    "\n",
    "        # Segunda Capa\n",
    "        self.conv2 = conv_k_3(channel1,channel2)\n",
    "        self.bn2 = nn.BatchNorm2d(channel2)\n",
    "\n",
    "        # Tercera Capa\n",
    "        self.conv3 = conv_k_3(channel2,channel3)\n",
    "        self.bn3 = nn.BatchNorm2d(channel3)\n",
    "\n",
    "        # Cuarta Capa\n",
    "        self.conv4 = conv_k_3(channel3,channel4)\n",
    "        self.bn4 = nn.BatchNorm2d(channel4)\n",
    "\n",
    "        # Max Pooling\n",
    "        self.max_pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "        self.fc = nn.Linear(in_features=8*8*channel4, out_features=10)\n",
    "        self.flatten = nn.Flatten()\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Aplicamos la primer capa convolusional, después hacemos batch normalization y finalmente su función de activación RELU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Aplicamos la segunda capa convolusional, después hacemos batch normalization y finalmente su finción de activación RELU\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        # Aplicamos Max Pooling después de este primer par de capas\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        # Aplicamos la tercer capa convolusional, después hacemos batch normalization y finalmente su función de activación RELU\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # Aplicamos la cuarta capa convolusional, después hacemos batch normalization y finalmente su finción de activación RELU\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "\n",
    "        # Aplicamos Max Pooling después de este segundo par de capas\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        #AGREGAMOS DROPOUT para evitar el sobreentrenamiento\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, costo: 1.1141241788864136, accuracy: 0.5822,\n",
      "Epoch: 2, costo: 1.13069748878479, accuracy: 0.6352,\n",
      "Epoch: 3, costo: 0.9112893342971802, accuracy: 0.6542,\n",
      "Epoch: 4, costo: 0.7465352416038513, accuracy: 0.7016,\n",
      "Epoch: 5, costo: 0.8618942499160767, accuracy: 0.7122,\n",
      "Epoch: 6, costo: 0.8661471009254456, accuracy: 0.7322,\n",
      "Epoch: 7, costo: 1.0242751836776733, accuracy: 0.7318,\n",
      "Epoch: 8, costo: 0.7354901432991028, accuracy: 0.7308,\n",
      "Epoch: 9, costo: 0.39422786235809326, accuracy: 0.7468,\n",
      "Epoch: 10, costo: 1.2691540718078613, accuracy: 0.7596,\n",
      "Epoch: 11, costo: 0.8837869763374329, accuracy: 0.7508,\n",
      "Epoch: 12, costo: 1.2687300443649292, accuracy: 0.7666,\n",
      "Epoch: 13, costo: 0.6549330353736877, accuracy: 0.7634,\n",
      "Epoch: 14, costo: 0.9818738698959351, accuracy: 0.767,\n",
      "Epoch: 15, costo: 0.4830458164215088, accuracy: 0.7666,\n",
      "Epoch: 16, costo: 0.7345582842826843, accuracy: 0.774,\n",
      "Epoch: 17, costo: 0.5041328072547913, accuracy: 0.7782,\n",
      "Epoch: 18, costo: 0.7752000689506531, accuracy: 0.7774,\n",
      "Epoch: 19, costo: 1.0875968933105469, accuracy: 0.7638,\n",
      "Epoch: 20, costo: 0.7898591756820679, accuracy: 0.78,\n",
      "Epoch: 21, costo: 0.5775021314620972, accuracy: 0.7752,\n",
      "Epoch: 22, costo: 1.0611939430236816, accuracy: 0.772,\n",
      "Epoch: 23, costo: 0.6507335901260376, accuracy: 0.7708,\n",
      "Epoch: 24, costo: 0.4630553722381592, accuracy: 0.7692,\n",
      "Epoch: 25, costo: 0.5106768012046814, accuracy: 0.7832,\n",
      "Epoch: 26, costo: 0.7315323352813721, accuracy: 0.7844,\n",
      "Epoch: 27, costo: 0.38050946593284607, accuracy: 0.7782,\n",
      "Epoch: 28, costo: 0.8651144504547119, accuracy: 0.7752,\n",
      "Epoch: 29, costo: 0.34942856431007385, accuracy: 0.7838,\n",
      "Epoch: 30, costo: 0.3906570076942444, accuracy: 0.792,\n"
     ]
    }
   ],
   "source": [
    "channel1 = 16   # Numero de filtros en el primer layer\n",
    "channel2 = 32   # Numero de filtros en la segunda layer\n",
    "channel3 = 64   # Numero de filtros en la tercera layer\n",
    "channel4 = 128   # Numero de filtros en la cuarta layer\n",
    "\n",
    "epochs = 30\n",
    "lr = 0.0001\n",
    "modelCNN2 = CNN_class2(3, channel1, channel2, channel3, channel4)\n",
    "optimiser = torch.optim.Adam(modelCNN2.parameters(), lr)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "train(modelCNN2, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7836"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver el accuracy con los datos de prueba\n",
    "accuracy(modelCNN2, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
